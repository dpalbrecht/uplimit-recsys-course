{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "837f0383-ee06-41a4-b431-032c431dfcbb",
      "metadata": {
        "id": "837f0383-ee06-41a4-b431-032c431dfcbb"
      },
      "source": [
        "# **Welcome to week 2 project!**\n",
        "\n",
        "Congratulations on making it to week 2! üëè In the first week of this course, we covered the basics of how to design personalized recommendation systems. We then provided some system design examples for large scale recommenders from corporations like Spotify and YouTube, as well as techniques for candidate generation, specifically the two-tower model being used at Twitter and Pinterest.\n",
        "\n",
        "This week, we covered details of ML approaches for recommendations: including multi-task recommenders and contextual bandits.\n",
        "\n",
        "In this week's project, we will touch upon both these methods at a high level. We first begin by developing a simple multi-task model, and then cover a contextual bandit model. For ease of use, we will re-use our H&M dataset from week 1 for Multi-task model, and will switch to using Yahoo! news dataset for contextual bandit model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a97fbf-e1ea-46f1-a355-fc1a4377a2ee",
      "metadata": {
        "id": "02a97fbf-e1ea-46f1-a355-fc1a4377a2ee"
      },
      "source": [
        "# **Multi-task Recommendations**\n",
        "\n",
        "Multi-task recommenders provide a way to predict multiple facets of user engagement in order to make comprehensive recommendations for content that users might like.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aeb2bd8-37fc-42f5-bc25-9c1eeb305d23",
      "metadata": {
        "id": "3aeb2bd8-37fc-42f5-bc25-9c1eeb305d23",
        "outputId": "73a48a51-1520-498c-a4c8-38ccd5fa636b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "text/html": [
              "<img src=\"https://miro.medium.com/max/3688/1*rrIJOpJO8fkFECNHlwq-jQ.png\" width=\"500\" class=\"unconfined\"/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import Image, display\n",
        "display(Image(url='https://miro.medium.com/max/3688/1*rrIJOpJO8fkFECNHlwq-jQ.png', width=500, unconfined=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c659f20-fe0d-4c07-a8bd-6f5b90be571d",
      "metadata": {
        "id": "5c659f20-fe0d-4c07-a8bd-6f5b90be571d"
      },
      "source": [
        "\n",
        "**Single task recommender:** we will start by deveoping a single task model that makes predictions about whether the user will purchase the article or not.\n",
        "\n",
        "**Multi-task recommender:** we then expand the scope to consider auxilliary prediction tasks, in order to improve the recommendation performance.\n",
        "\n",
        "Specifically, we make the folliwing three predictions using a multi-task setup:\n",
        "1. Predicting whether the user will make any purchase next week or not\n",
        "2. Predicting which category the user will make purchase in\n",
        "3. Predicting which specific item the user will buy\n",
        "\n",
        "We will use different loss functions to jointly train the multi-task network.\n",
        "\n",
        "**Negative Sampling**\n",
        "It is important to note that the dataset is only of positive cases -- cases wherein the user bought an article. To train our models, we will have to resort to negative sampling to develop a mixed dataset comprising of positive and negative examples.\n",
        "\n",
        "We will cover two categories of negative sampling:\n",
        "1. Random negative sampling\n",
        "2. Negative sampling with bias\n",
        "\n",
        "**Evaluation:**\n",
        "We will evaluate the ranker performance on various ranking metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "add75169-0279-469c-98c2-36e6fc28ebdd",
      "metadata": {
        "id": "add75169-0279-469c-98c2-36e6fc28ebdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518b44eb-a8c9-4c03-da4e-939183e2a4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88 kB 3.6 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 511.7 MB 5.6 kB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 438 kB 54.9 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.8 MB 35.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.6 MB 52.6 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.3 MB 5.6 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.2 MB 5.1 MB/s \n",
            "\u001b[?25hDownloading...\n",
            "From: https://drive.google.com/uc?id=1AzGFgJjNdlQ8LI5PXbeuxr21tyJinkv0\n",
            "To: /content/hmdata.zip\n",
            "100% 773M/773M [00:05<00:00, 154MB/s]\n",
            "Archive:  hmdata.zip\n",
            "   creating: hmdata/\n",
            "  inflating: hmdata/customers.csv.zip  \n",
            "  inflating: __MACOSX/hmdata/._customers.csv.zip  \n",
            "  inflating: hmdata/articles.csv.zip  \n",
            "  inflating: __MACOSX/hmdata/._articles.csv.zip  \n",
            "  inflating: hmdata/transactions_train.csv.zip  \n",
            "  inflating: __MACOSX/hmdata/._transactions_train.csv.zip  \n",
            "  inflating: hmdata/sample_submission.csv.zip  \n",
            "  inflating: __MACOSX/hmdata/._sample_submission.csv.zip  \n"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-recommenders --upgrade\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "!pip install -q scann\n",
        "!gdown https://drive.google.com/uc?id=1AzGFgJjNdlQ8LI5PXbeuxr21tyJinkv0\n",
        "!unzip hmdata.zip\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "from typing import Dict, Text\n",
        "from functools import reduce\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "from typing import Dict, Text\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from tensorflow import feature_column\n",
        "import random\n",
        "from datetime import timedelta\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3081bc7b-ab90-41c0-9c21-35dfe59b4228",
      "metadata": {
        "id": "3081bc7b-ab90-41c0-9c21-35dfe59b4228"
      },
      "outputs": [],
      "source": [
        "article_df = pd.read_csv(\"hmdata/articles.csv.zip\")\n",
        "customer_df = pd.read_csv(\"hmdata/customers.csv.zip\")\n",
        "train0 = pd.read_csv('hmdata/transactions_train.csv.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b073d71-0ffc-407c-9338-1a87cee00b07",
      "metadata": {
        "id": "2b073d71-0ffc-407c-9338-1a87cee00b07"
      },
      "source": [
        "# **Simple Feature Processing**\n",
        "\n",
        "In this section we focus on extracting features for trainnig the ranking models. We focus primarily on extracting user features, and article features, and leave sophisticated feature modelin for next week.\n",
        "\n",
        "For users specifically, we will focus on extracting the following features:\n",
        "1. age bucket\n",
        "2. no of past purchases\n",
        "3. min/max/avg price\n",
        "4. distrbution over product group\n",
        "5. distrbution over section\n",
        "\n",
        "For aticles, we will learn embedding based features for the article category, product group, and department.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test\n",
        "\n",
        "N_DAYS_TRAIN = 30\n",
        "N_DAYS_TEST = 7\n",
        "\n",
        "\n",
        "max_date = train0['t_dat'].max()\n",
        "train = train0[(train0['t_dat']>=((pd.to_datetime(max_date) - timedelta(days=N_DAYS_TRAIN)).date().strftime('%Y-%m-%d')))\n",
        "                & (train0['t_dat']<((pd.to_datetime(max_date) - timedelta(days=N_DAYS_TEST)).date().strftime('%Y-%m-%d')))]\n",
        "test = train0[(train0['t_dat']>=((pd.to_datetime(max_date) - timedelta(days=N_DAYS_TEST)).date().strftime('%Y-%m-%d')))]\n",
        "\n",
        "del train0\n",
        "gc.collect()\n",
        "\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4uIrlIcIVFG",
        "outputId": "d3a49e3a-eaf0-403e-ad2d-98207f6f8247"
      },
      "id": "a4uIrlIcIVFG",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((889569, 5), (266364, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract customer features using train\n",
        "# Join them to both train and test\n",
        "\n",
        "train = train.merge(article_df, on='article_id', how='outer')\n",
        "test = test.merge(article_df, on='article_id', how='outer')\n",
        "\n",
        "f1 = train.groupby(\"customer_id\", as_index=False).article_id.nunique() #unique articles\n",
        "f2 = train.groupby(\"customer_id\", as_index=False).t_dat.nunique() #days active\n",
        "f3 = train.groupby(\"customer_id\", as_index=False).size() #total purchases\n",
        "f4 = train.groupby(\"customer_id\", as_index=False).price.min() #min price\n",
        "f5 = train.groupby(\"customer_id\", as_index=False).price.max() #max price\n",
        "f6 = train.groupby(\"customer_id\", as_index=False).price.mean() #mean price\n",
        "f7 = train.groupby(\"customer_id\", as_index=False).product_group_name.nunique() #num unique product groups\n",
        "f8 = train.groupby(\"customer_id\", as_index=False).department_name.nunique() #num unique department_name\n",
        "f9 = train.groupby(\"customer_id\", as_index=False).section_name.nunique() #num unique section_name\n",
        "\n",
        "data_frames = [f1,f2,f3,f4,f5,f6,f7,f8,f9]\n",
        "customer_features_agg_df = reduce(lambda  left,right: pd.merge(left, right, on=['customer_id'], how='outer'), data_frames)\n",
        "customer_features_agg_df.columns = ['customer_id', 'nArticles', 'nDays', 'nPurchases', 'minPrice', 'maxPrice', 'meanPrice', 'nPGroups', 'nDept', 'nSect']\n",
        "\n",
        "train = train.merge(customer_features_agg_df, on='customer_id', how='outer')\n",
        "test = test.merge(customer_features_agg_df, on='customer_id', how='outer')"
      ],
      "metadata": {
        "id": "Ni7S-8nPKusO"
      },
      "id": "Ni7S-8nPKusO",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove records where we don't have all customer features\n",
        "train = train[~train['customer_id'].isnull()]\n",
        "test = test[~test['customer_id'].isnull()]\n",
        "\n",
        "# Use 40k customers from train only so we can quickly test our methods\n",
        "CUSTOMER_IDS = train['customer_id'].unique()[:40000]\n",
        "train = train[train['customer_id'].isin(CUSTOMER_IDS)]\n",
        "test = test[test['customer_id'].isin(CUSTOMER_IDS)]\n",
        "\n",
        "train.shape, test.shape, train['customer_id'].nunique(), test['customer_id'].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvwN7KRvjb0a",
        "outputId": "96f3dd3a-784e-45a4-b31e-be5b6f6a150c"
      },
      "id": "AvwN7KRvjb0a",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((282405, 38), (61287, 38), 40000, 40000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff60881-c2ab-4c1f-b69a-67c28becb6cf",
      "metadata": {
        "id": "9ff60881-c2ab-4c1f-b69a-67c28becb6cf"
      },
      "source": [
        "# Negative Sampling for training ranker\n",
        "\n",
        "If you notice the dataset, we only have purchase information. This is generallyt he case -- we often only observe positive interactions from users. However, to train a model, we would need access to positive and negative samples both.\n",
        "\n",
        "To generate negative sampled, here we focus on random sampling of negative examples per user-artice transaction. Other methods exist to sample negatives, namely:\n",
        "1. Random sampling\n",
        "2. Biased sampling from similar categories\n",
        "3. Cross-batch negative sampling\n",
        "\n",
        "In this project, we will go ahead with the random sampling strategy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate N negative samples per customer\n",
        "\n",
        "def generate_negative_samples(df, article_list, num_neg=1):\n",
        "    negative_df = df.groupby(\"customer_id\", as_index=False).article_id.agg(['unique']).reset_index()\n",
        "    negative_df.columns = ['customer_id', 'article_list']\n",
        "\n",
        "    random_sample_size = (len(negative_df), negative_df['article_list'].map(len).max()+num_neg)\n",
        "    random_samples = np.random.choice(article_list, \n",
        "                                      size=random_sample_size,\n",
        "                                      replace=True)\n",
        "    negData = []\n",
        "    for row_n, (index, row) in tqdm(enumerate(negative_df.iterrows()), total=len(negative_df)):\n",
        "        count, temp_negs = 0, []\n",
        "        for n in random_samples[row_n]:\n",
        "            if (n not in row['article_list']) & (n not in temp_negs):\n",
        "                negData.append([row['customer_id'], n])\n",
        "                temp_negs.append(n)\n",
        "                count += 1\n",
        "            if count == num_neg:\n",
        "                break\n",
        "    negDf = pd.DataFrame(negData, columns=['customer_id', 'article_id'])\n",
        "    if len(negDf) != negative_df['customer_id'].nunique()*num_neg:\n",
        "        print('WARNING: Not enough samples generated. Increase size of negative sample size or try again.')\n",
        "    return negDf\n",
        "\n",
        "\n",
        "articlesSet = list(train.article_id.unique())\n",
        "train_neg_df = generate_negative_samples(train, articlesSet, num_neg=2)\n",
        "test_neg_df = generate_negative_samples(test, articlesSet, num_neg=2)\n",
        "\n",
        "train_neg_df.shape, test_neg_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "7a54c3793c5f4e1baef6f49bc6c03135",
            "970414b4a58c4239b89cb28fdc84b200",
            "d57a547d456d441b967a61dce5e0c03f",
            "7758f9acf37041deace83a93d4741e45",
            "885318011acb4a21a2b5891ef29a7b85",
            "d17f08e7c0dc44739e49f87c0c358973",
            "649ca2144ede4338b50006c9473907fc",
            "278e7447d8b240628eb1044a7a44d3aa",
            "6fd8d734fd3c4194a0dd1e9d540608fc",
            "c77eb115198e47e798bc6b0a305f9b5d",
            "80046eb2d2fb40b683314759177bbafe",
            "cbb01d2b74374d8e85a03a1545285700",
            "0211cc97a40d4fa7b00d8b382b088387",
            "dead86576d55423bbc704d7708dd0086",
            "20c7132abb49432eae782547e5a6dd2b",
            "bcefc03e7ea341bbb3e361c79dbefb93",
            "22a2dd67b9dc4d7499782ec56ecbae7c",
            "8d96b8b4a42b437bab13bffa49c19581",
            "bfb5fc60fc524d3fad01507296147cff",
            "9d8b67018c364bdfbfa16728edc0bc4f",
            "37b6ef386dd94847a5f1bedc72d8a635",
            "edc0167e788b4118a129ea389ed85fb8"
          ]
        },
        "id": "QUS_Baq8ND0i",
        "outputId": "8fe1cee4-689b-44f6-afa9-01fc7ea08904"
      },
      "id": "QUS_Baq8ND0i",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/40000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a54c3793c5f4e1baef6f49bc6c03135"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/40000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbb01d2b74374d8e85a03a1545285700"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((80000, 2), (80000, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affe3023-ad6e-4c30-b02d-df5626b479a5",
      "metadata": {
        "id": "affe3023-ad6e-4c30-b02d-df5626b479a5"
      },
      "source": [
        "### We next generate features for negatives"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine positive and negative samples\n",
        "# Generate label for purchase or not (label1)\n",
        "\n",
        "def add_labels_and_combine(pos_df, neg_df):\n",
        "    # Add article and customer features to negatives\n",
        "    neg_df = neg_df.merge(article_df[['article_id','product_group_name', 'department_name','section_name']], on='article_id', how='left')\n",
        "    neg_df = neg_df.merge(customer_features_agg_df, on='customer_id', how='left')\n",
        "    # These are the positive purchases\n",
        "    pos_df['label1'] = 1\n",
        "    # These are the random, negative purchases\n",
        "    neg_df['label1'] = 0\n",
        "    # Combine positives and negatives\n",
        "    return pd.concat([pos_df[pos_df['customer_id'].isin(CUSTOMER_IDS)], neg_df])\n",
        "\n",
        "\n",
        "combined_train = add_labels_and_combine(train, train_neg_df)\n",
        "combined_test = add_labels_and_combine(test, test_neg_df)\n",
        "\n",
        "combined_train.shape, combined_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDW06FMBPGjO",
        "outputId": "ba640c61-912a-47ec-9c2d-fdf19643b4d4"
      },
      "id": "EDW06FMBPGjO",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((362405, 39), (141287, 39))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857e7128-8eee-4b15-a6dd-14b8ba62301c",
      "metadata": {
        "id": "857e7128-8eee-4b15-a6dd-14b8ba62301c"
      },
      "source": [
        "## Single Task Ranker\n",
        "\n",
        "We begin by training a single task ranker for the task of predicting whether the user will purchase a given article or not."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use customer features and product_group_name to predict whether the customer bought the product or not\n",
        "\n",
        "numFeatures = ['nArticles','nDays','nPurchases','minPrice','maxPrice','meanPrice','nPGroups','nDept','nSect']\n",
        "X1 = np.asarray(combined_train[numFeatures].values)\n",
        "X2 = combined_train['product_group_name'].astype(str)\n",
        "y1 = np.asarray(combined_train['label1'].values)\n",
        "\n",
        "\n",
        "# Numerical features\n",
        "inp1 = tf.keras.Input((len(numFeatures)))\n",
        "x1 = tf.keras.layers.Dense(50, activation='relu')(inp1)\n",
        "\n",
        "# Categorical features\n",
        "unique_group_names = combined_train['product_group_name'].unique().astype(str)\n",
        "inp2 = tf.keras.Input((1, ), dtype=tf.string)\n",
        "x2 = tf.keras.layers.StringLookup(vocabulary=unique_group_names, mask_token=None)(inp2)\n",
        "x2 = tf.keras.layers.Embedding(len(unique_group_names) + 1, 50)(x2)\n",
        "x2 = tf.keras.layers.Dense(32, activation='relu')(x2)\n",
        "x2 = tf.keras.layers.Flatten()(x2)\n",
        "\n",
        "# Combine inputs\n",
        "x12 = tf.keras.layers.concatenate([x1,x2])\n",
        "x12 = tf.keras.layers.Dense(50)(x12)\n",
        "out1 = tf.keras.layers.Dense(1, activation='sigmoid')(x12)\n",
        "\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "metric = tf.keras.metrics.BinaryAccuracy()\n",
        "m = tf.keras.Model([inp1, inp2], out1)\n",
        "m.compile(loss=bce,\n",
        "          optimizer='adam',\n",
        "          metrics=[metric])\n",
        "\n",
        "m.fit([X1, X2], [y1], epochs=5, verbose=1, batch_size=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0Ywx6Ais9hS",
        "outputId": "e16b6466-cef1-416f-d948-40d914efbb0e"
      },
      "id": "l0Ywx6Ais9hS",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "363/363 [==============================] - 5s 10ms/step - loss: 0.4648 - binary_accuracy: 0.7975\n",
            "Epoch 2/5\n",
            "363/363 [==============================] - 4s 12ms/step - loss: 0.4594 - binary_accuracy: 0.8010\n",
            "Epoch 3/5\n",
            "363/363 [==============================] - 3s 9ms/step - loss: 0.4587 - binary_accuracy: 0.8010\n",
            "Epoch 4/5\n",
            "363/363 [==============================] - 3s 9ms/step - loss: 0.4579 - binary_accuracy: 0.8012\n",
            "Epoch 5/5\n",
            "363/363 [==============================] - 3s 9ms/step - loss: 0.4579 - binary_accuracy: 0.8013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e59678250>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set\n",
        "\n",
        "def evaluate(df):\n",
        "    X1 = np.asarray(df[numFeatures].values)\n",
        "    X2 = df['product_group_name'].astype(str)\n",
        "    preds = m.predict([X1, X2])\n",
        "    preds = (preds>=0.5).astype(int).flatten()\n",
        "    print(sum(preds == df['label1'].values)/len(preds))\n",
        "\n",
        "\n",
        "evaluate(combined_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFVvgZ0UtuvO",
        "outputId": "01e67255-dabe-4c76-deb3-9f0d48c3cce0"
      },
      "id": "LFVvgZ0UtuvO",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4416/4416 [==============================] - 12s 3ms/step\n",
            "0.49096519849667697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ada5f3-8157-4088-95b8-8e17989b2d52",
      "metadata": {
        "id": "65ada5f3-8157-4088-95b8-8e17989b2d52"
      },
      "source": [
        "### **From single to multi-task recommendation model**\n",
        "We can now add another task to enrich the single task model to multi-task model. To do so, we first have to decide what the task 2 would be.\n",
        "\n",
        "By looking at the data we have, we hypothesize that being able to predict whether or not a customer would purchase an article from certain product category would be a good learnt model to have. Indeed, being able to predict which category would the user purchase in would help us narrow down the list of recommendations we want to surface infront of users.\n",
        "\n",
        "Based on this intuition, we want to develop a multi-task model with 2 tasks:\n",
        "1. Task 1: predict whether or not a user would purchase an article\n",
        "2. Task 2: predict whether or not a user would purchase any article from a given category\n",
        "\n",
        "The training data we have created thus far is solely based on prediction task 1: predicting whether or not a user would purchase a given article. Corredpondingly, in trainDF dataframe we have collected positive and nagetive examples for this task, and have added a label1 column as the final label to train the model.\n",
        "\n",
        "To develop a multi-task model for these two tasks, we will now need to add a label for the second task in the same dataset: trainDF. To do so, we will need to add a column: label2 to this dataframe -- for each row, we will need to identify whether the user has made any purchase in this category or not, and if the user has made a purchase in this category, then we will assign label2 = 1, else label2 = 0. Please note that for the same training example, it may happen that label1 = 0 but label2 = 1; this would happen in cases wherein the customer did not purchase this specific article but did purchase any other article from this category.\n",
        "\n",
        "The goal for this project is to write the function that adds this label2 column to the trainDF dataframe, to enable us to train a multi-task model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "39b6f339-ccae-4cb8-949d-dc7947333bf1",
      "metadata": {
        "id": "39b6f339-ccae-4cb8-949d-dc7947333bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "150065b0-a84c-450b-9932-5d93aabed7da"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1    323756\n",
              "0     38649\n",
              "Name: label2, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    78539\n",
              "1    62748\n",
              "Name: label2, dtype: int64"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Add the second label for multi-task learning (whether the user would purchase an article from that product_group_name)\n",
        "\n",
        "customer2productgroups = dict(train.groupby('customer_id')['product_group_name'].apply(lambda x: set(list(x))))\n",
        "\n",
        "def add_label2_to_trainDF(x):\n",
        "    \"\"\"\n",
        "    write code here to add a new column to trainDF\n",
        "    the new column would be named label2 and it represents the label for the task 2\n",
        "    \n",
        "    After this function has been run, it can be safely assumed that trainDF dataframe contains another column called \"label2\"\n",
        "    that describes the label for the second prediction task.\n",
        "    \"\"\"\n",
        "    purchased_groups = customer2productgroups[x['customer_id']]\n",
        "    purchased_this_group = int(x['product_group_name'] in purchased_groups)\n",
        "    return purchased_this_group\n",
        "\n",
        "combined_train['label2'] = combined_train.apply(add_label2_to_trainDF, axis=1)\n",
        "combined_test['label2'] = combined_test.apply(add_label2_to_trainDF, axis=1)\n",
        "\n",
        "display(combined_train['label2'].value_counts(dropna=False))\n",
        "display(combined_test['label2'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2642a24-090e-4180-9be0-e9bb27a89695",
      "metadata": {
        "id": "f2642a24-090e-4180-9be0-e9bb27a89695"
      },
      "source": [
        "Now that we have both the label1 and label2 in our dataset, we can construct a simple multi-task model with shared bottom, which we call the common layers. Building on top of the common layers, we will have two separate modules: task1 layers and task2 layers. Layers of task 1 (\"task1a\" and \"task1b\") are dedicated to learning weights that help us perform better on task1. Correspondingly, layers of task 2 (\"task2a\" and \"task2b\") are dedicated to learning weights that help us perform better on task2.\n",
        "\n",
        "Below we implement such a multi-task model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the same features as above, train using the multi-task method\n",
        "\n",
        "numFeatures = ['nArticles','nDays','nPurchases','minPrice','maxPrice','meanPrice','nPGroups','nDept','nSect']\n",
        "X1 = np.asarray(combined_train[numFeatures].values)\n",
        "X2 = combined_train['product_group_name'].astype(str)\n",
        "y1 = np.asarray(combined_train['label1'].values)\n",
        "y2 = np.asarray(combined_train['label2'].values)\n",
        "\n",
        "\n",
        "# Numerical features\n",
        "inp1 = tf.keras.Input((len(numFeatures)))\n",
        "x1 = tf.keras.layers.Dense(50, activation='relu')(inp1)\n",
        "\n",
        "# Categorical features\n",
        "unique_group_names = combined_train['product_group_name'].unique().astype(str)\n",
        "inp2 = tf.keras.Input((1, ), dtype=tf.string)\n",
        "x2 = tf.keras.layers.StringLookup(vocabulary=unique_group_names, mask_token=None)(inp2)\n",
        "x2 = tf.keras.layers.Embedding(len(unique_group_names) + 1, 50)(x2)\n",
        "x2 = tf.keras.layers.Dense(32, activation='relu')(x2)\n",
        "x2 = tf.keras.layers.Flatten()(x2)\n",
        "\n",
        "# Combine inputs\n",
        "common = tf.keras.layers.concatenate([x1,x2])\n",
        "common = tf.keras.layers.Dense(50)(common)\n",
        "\n",
        "# Task 1\n",
        "x1 = tf.keras.layers.Dense(32, activation='relu', name=\"task1a\")(common)\n",
        "x1 = tf.keras.layers.Dense(16, activation='relu', name=\"task1b\")(x1)\n",
        "out1 = tf.keras.layers.Dense(1, activation='sigmoid', name=\"out1\")(x1)\n",
        "\n",
        "# Task 2\n",
        "x2 = tf.keras.layers.Dense(32, activation='relu', name=\"task2a\")(common)\n",
        "x2 = tf.keras.layers.Dense(16, activation='relu', name=\"task2b\")(x2)\n",
        "out2 = tf.keras.layers.Dense(1, activation='sigmoid', name=\"out2\")(x2)\n",
        "\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "metric = tf.keras.metrics.BinaryAccuracy()\n",
        "m2 = tf.keras.Model([inp1, inp2], [out1,out2])\n",
        "m2.compile(loss=bce,\n",
        "          optimizer='adam',\n",
        "          metrics=[metric])\n",
        "\n",
        "m2.fit([X1, X2], [y1,y2], epochs=5, verbose=1, batch_size=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmBi6_QV2OmX",
        "outputId": "5b61d48f-90e6-4f69-f6a3-4d6fa7ee1737"
      },
      "id": "xmBi6_QV2OmX",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "363/363 [==============================] - 7s 12ms/step - loss: 0.6897 - out1_loss: 0.4578 - out2_loss: 0.2319 - out1_binary_accuracy: 0.7997 - out2_binary_accuracy: 0.9098\n",
            "Epoch 2/5\n",
            "363/363 [==============================] - 5s 13ms/step - loss: 0.6575 - out1_loss: 0.4447 - out2_loss: 0.2127 - out1_binary_accuracy: 0.8079 - out2_binary_accuracy: 0.9160\n",
            "Epoch 3/5\n",
            "363/363 [==============================] - 5s 15ms/step - loss: 0.6526 - out1_loss: 0.4423 - out2_loss: 0.2103 - out1_binary_accuracy: 0.8094 - out2_binary_accuracy: 0.9172\n",
            "Epoch 4/5\n",
            "363/363 [==============================] - 4s 12ms/step - loss: 0.6498 - out1_loss: 0.4409 - out2_loss: 0.2089 - out1_binary_accuracy: 0.8098 - out2_binary_accuracy: 0.9177\n",
            "Epoch 5/5\n",
            "363/363 [==============================] - 5s 13ms/step - loss: 0.6477 - out1_loss: 0.4400 - out2_loss: 0.2077 - out1_binary_accuracy: 0.8107 - out2_binary_accuracy: 0.9181\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e8cbf7390>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set\n",
        "\n",
        "def evaluate_multi_task(df):\n",
        "    X1 = np.asarray(df[numFeatures].values)\n",
        "    X2 = df['product_group_name'].astype(str)\n",
        "    preds, _ = m2.predict([X1, X2])\n",
        "    preds = (preds>=0.5).astype(int).flatten()\n",
        "    print(sum(preds == df['label1'].values)/len(preds))\n",
        "\n",
        "\n",
        "evaluate_multi_task(combined_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut63jiFb2aIj",
        "outputId": "6a64fd62-d1d0-466f-e5b7-36899e7ddce9"
      },
      "id": "Ut63jiFb2aIj",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4416/4416 [==============================] - 12s 3ms/step\n",
            "0.5241954319930354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5866181-5319-472a-8b85-0004fa8c652e",
      "metadata": {
        "id": "f5866181-5319-472a-8b85-0004fa8c652e"
      },
      "source": [
        "### Checkpoint & goals\n",
        "By now you should have implemented the function **def add_label2_to_trainDF()** and replaced label1 in the above code by label2 and trained the multi-task model with these 2 tasks.\n",
        "\n",
        "Please record the performance of both the single task model and multi-task model and provide your interpretation of the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d030b3d7-9323-4ee1-8cf8-cc99c2048b44",
      "metadata": {
        "id": "d030b3d7-9323-4ee1-8cf8-cc99c2048b44"
      },
      "outputs": [],
      "source": [
        "# Both methods are much too overfit to be useful, as shown by testing on the held-out test set, \n",
        "# but the multi-task model does show improvement over the single-task ranker and at least shows an indication of being better than random.\n",
        "# I suspect, however, with more positive and negative samples we could easily get the model to fit more appropriately and have a useful model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0478c09-91fa-404d-84f5-34ae7fe9f359",
      "metadata": {
        "id": "d0478c09-91fa-404d-84f5-34ae7fe9f359"
      },
      "source": [
        "## Freezing layers for secondary tasks\n",
        "\n",
        "Often when training a multi-task model, one needs to freeze parts of the nueral network and only train one specific task module, while keeping the weights of the other task module fixed. In Keras we can do this by freezing layers. The below example demonstrates how one could go about freezing layers in a neural network, and train only the trainable parts of a nueral network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3b40d34e-fa1c-4e09-b29a-7df832a3746b",
      "metadata": {
        "id": "3b40d34e-fa1c-4e09-b29a-7df832a3746b",
        "outputId": "c0d94ec5-516b-4127-e41e-5b2ec1c71deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.functional.Functional object at 0x7f4e82b54e50> True\n",
            "<keras.engine.input_layer.InputLayer object at 0x7f4e7e1eefd0> True\n",
            "<keras.layers.preprocessing.string_lookup.StringLookup object at 0x7f4e80e1c490> True\n",
            "<keras.layers.core.embedding.Embedding object at 0x7f4eadb45350> True\n",
            "<keras.engine.input_layer.InputLayer object at 0x7f4e6f8cb6d0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e832860d0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e7e1ee690> True\n",
            "<keras.layers.reshaping.flatten.Flatten object at 0x7f4e8a877d90> True\n",
            "<keras.layers.merging.concatenate.Concatenate object at 0x7f4e8a877f50> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e82b1e1d0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e5b012d90> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e81a2fc50> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e83286590> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e5b012fd0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e81a2fe10> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e82b26a50> True\n"
          ]
        }
      ],
      "source": [
        "# lets print the training status of each layer of the models we have\n",
        "for k,v in m2._get_trainable_state().items():\n",
        "    print(k,v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4842f73e-3367-4b47-890d-93936b18b212",
      "metadata": {
        "id": "4842f73e-3367-4b47-890d-93936b18b212"
      },
      "outputs": [],
      "source": [
        "# Lets try to freeze layers for task 2 so that no training example affects the module dedicated to task 2\n",
        "for layer in m2.layers:\n",
        "    if layer.name in [\"task2a\",\"task2b\"]:\n",
        "        layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2b01277c-eb22-47b7-8bb7-7d1a106543c8",
      "metadata": {
        "id": "2b01277c-eb22-47b7-8bb7-7d1a106543c8",
        "outputId": "7d8142cb-ef19-4fea-9a44-d61c12ed19a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.functional.Functional object at 0x7f4e82b54e50> True\n",
            "<keras.engine.input_layer.InputLayer object at 0x7f4e7e1eefd0> True\n",
            "<keras.layers.preprocessing.string_lookup.StringLookup object at 0x7f4e80e1c490> True\n",
            "<keras.layers.core.embedding.Embedding object at 0x7f4eadb45350> True\n",
            "<keras.engine.input_layer.InputLayer object at 0x7f4e6f8cb6d0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e832860d0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e7e1ee690> True\n",
            "<keras.layers.reshaping.flatten.Flatten object at 0x7f4e8a877d90> True\n",
            "<keras.layers.merging.concatenate.Concatenate object at 0x7f4e8a877f50> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e82b1e1d0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e5b012d90> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e81a2fc50> False\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e83286590> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e5b012fd0> False\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e81a2fe10> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f4e82b26a50> True\n"
          ]
        }
      ],
      "source": [
        "for k,v in m2._get_trainable_state().items():\n",
        "    print(k,v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9e9b0509-3d51-46a6-84be-7a2069d0ad5f",
      "metadata": {
        "id": "9e9b0509-3d51-46a6-84be-7a2069d0ad5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20bb101f-2478-47d0-ba29-64372b9951e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " string_lookup_1 (StringLookup)  (None, 1)           0           ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 1, 50)        850         ['string_lookup_1[0][0]']        \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 9)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 1, 32)        1632        ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 50)           500         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 32)           0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 82)           0           ['dense_4[0][0]',                \n",
            "                                                                  'flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 50)           4150        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " task1a (Dense)                 (None, 32)           1632        ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " task2a (Dense)                 (None, 32)           1632        ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " task1b (Dense)                 (None, 16)           528         ['task1a[0][0]']                 \n",
            "                                                                                                  \n",
            " task2b (Dense)                 (None, 16)           528         ['task2a[0][0]']                 \n",
            "                                                                                                  \n",
            " out1 (Dense)                   (None, 1)            17          ['task1b[0][0]']                 \n",
            "                                                                                                  \n",
            " out2 (Dense)                   (None, 1)            17          ['task2b[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,486\n",
            "Trainable params: 9,326\n",
            "Non-trainable params: 2,160\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "m2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48cccb53-490d-48fe-89f9-99895bdfc50d",
      "metadata": {
        "id": "48cccb53-490d-48fe-89f9-99895bdfc50d"
      },
      "source": [
        "## Tasks for Week 2 project:\n",
        "\n",
        "So glad that you have made it this far!! The above gives a high level view on how one could train single, and multi-task models and how one could go about freezing layers and training parts of a multi-task network.\n",
        "\n",
        "Lets re-iterate the main goal for this part of the project 2:\n",
        "\n",
        "- Goal: Implement the function that assigns label2 to the trainDF dataframe, and run the single task and multi-tsk models without layer freezing. Once done, compare the performance metrics for task 1 from both these models. Comment on whether the addition of second task is adding value to predictive power of first task.\n",
        "\n",
        "Optional: you can notice that we have used only numeric features in the model. Feel free to add other categorical features and re-train the model to see if the performance increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649306f5-6cd8-4339-9921-41f6de1dc948",
      "metadata": {
        "id": "649306f5-6cd8-4339-9921-41f6de1dc948"
      },
      "outputs": [],
      "source": [
        "# Answered above:\n",
        "\n",
        "# Both methods are much too overfit to be useful, as shown by testing on the held-out test set, \n",
        "# but the multi-task model does show improvement over the single-task ranker and at least shows an indication of being better than random.\n",
        "# I suspect, however, with more positive and negative samples we could easily get the model to fit more appropriately and have a useful model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21b257c-d1e8-42c8-8926-86b8ee5ff88b",
      "metadata": {
        "id": "d21b257c-d1e8-42c8-8926-86b8ee5ff88b"
      },
      "source": [
        "# Contextual Bandits for Recommendations\n",
        "\n",
        "We can also use contextual bandits to produce personalized recommendations. Compared to multi-task recommenders, contextual bandits provide a balance between exploring new options and exploiting known information to find potential recommendations.\n",
        "\n",
        "## Background: multi-arm bandits\n",
        "The multi-armed bandit problem is an example of one-step reinforcement learning. To explore an example, lets assume we have a gambler who is provided a slot machine with multiple arms, each with its own unknown probability distribution of payouts. The objective is to pull the arms one-by-one in a sequence while gathering information in order to maximize the total payout over the long-run. The multi-armed bandit problem can be seen as a toy problem for reinforcement learning with one step rollout. Namely we have a game consisting of n rounds and in each round t:\n",
        "\n",
        "1. Player selects one of K actions (think of slot machines and pulling their arms, hence the name).\n",
        "2. Player gets reward of Rt . Each action i ‚àà {1,2,‚Ä¶,K } has a fixed, but unknown to the player, reward distribution Pi with the expected reward Œºi .\n",
        "3. Given the history of actions and rewards Player updates their strategy.\n",
        "\n",
        "Given the history of actions and rewards, the player updates their strategy. The goal of the player is to maximize the reward, which naturally has to be done by exploring new options (and thus learning about other machines' distributions) and exploiting known actions that proved to give high rewards so far. Mathematically the player wishes to minimize the regret.\n",
        "\n",
        "## Contextual multi-arm bandits\n",
        "The contextual bandit problem is a generalization of the multi-armed bandit that extends the model by making actions conditional on the state of the environment. Unlike the classical multi-armed bandit, it addresses the problem of identifying the most appropriate content given all relevant contextual signals.\n",
        "\n",
        "1. Player gets context xt .\n",
        "2. Player selects at ‚àà A .\n",
        "3. Player gets reward Rt(at) .\n",
        "4. Given the history of actions, rewards and contexts Player updates their strategy.\n",
        "\n",
        "A common real-world contextual bandit example is a recommendation system. Given a set of presented articles, a reward is determined by the click-through behavior of the user. If user clicks on the article, a payout of 1 is incurred and 0 otherwise. Click-through-rate (CRT) is used to determine the selection and placement of ads within the recommendation application.\n",
        "\n",
        "\n",
        "In explore phase the algorithm, given its internal estimates of goodness of each actions chooses the best possible action modulo exploration. For example, epsilon-greedy chooses the best action with probability (1‚àíœµ) and with probability œµ it selects uniformly one of all possible actions.\n",
        "\n",
        "In the learning phase, the algorithm, after it selects an action and obtains a reward, updates its estimates of expected rewards of actions given the context. This is usually done via feeding a single example consisting of x,a,r (context, action, reward) to an estimator model doing a single update step (batch size 1). The learn phase is usually common for most algorithms and the main differences come from the exploration phase. Some algorithms include œµ-greedy, ensemble of policies or approximations to Lin-UCB. \n",
        "œµ-greedy tends to be a practical choice for initial experiments since it‚Äôs straightforward to understand and control its exploration. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc685243-6acc-460e-b6ba-15aa265daf97",
      "metadata": {
        "id": "dc685243-6acc-460e-b6ba-15aa265daf97"
      },
      "source": [
        "## Dataset: R6A - Yahoo! Front Page Today Module User Click Log Dataset, version 1.0 (1.1 GB)\n",
        "\n",
        "Our dataset contains a fraction of user click log for news articles displayed in the Featured Tab of the Today Module on Yahoo! Front Page (http://www.yahoo.com) during the first ten days in May 2009.  The articles were chosen uniformly at random from a hand-picked pool of high-quality articles, which allows one to use a recently developed method to obtain an unbiased evaluation of an arbitrary bandit algorithm.\n",
        "\n",
        "The dataset contains 45,811,883 user visits to the Today Module.  For each visit, both the user and each of the candidate articles are\n",
        "associated with a feature vector of dimension 6 (including a constant feature), constructed by conjoint analysis with a bilinear model.\n",
        "\n",
        "This dataset contains 10 files, corresponding to the first 10 days in May 2009:\n",
        "    ydata-fp-td-clicks-v1_0.20090501.gz\n",
        "    ydata-fp-td-clicks-v1_0.20090502.gz\n",
        "    ...\n",
        "    ydata-fp-td-clicks-v1_0.20090510.gz\n",
        "Each line in the files corresponds to a separate user visit.  An example line is as follows:\n",
        "\n",
        "1241160900 109513 0 |user 2:0.000012 3:0.000000 4:0.000006 5:0.000023 6:0.999958 1:1.000000 |109498 2:0.306008 3:0.000450 4:0.077048 5:0.230439 6:0.386055 1:1.000000 |109509 2:0.306008 3:0.000450 4:0.077048 5:0.230439 6:0.386055 1:1.000000 [[...more article features omitted...]] |109453 2:0.421669 3:0.000011 4:0.010902 5:0.309585 6:0.257833 1:1.000000\n",
        "\n",
        "which contains the following fields delimited with spaces:\n",
        "\n",
        "    * timestamp: e.g., 1241160900\n",
        "    * displayed_article_id: e.g., 109513\n",
        "    * user_click (0 for no-click and 1 for click): e.g., 0\n",
        "    * strings \"|user\" and \"|{article_id}\" indicate the start of user\n",
        "      and article features\n",
        "    * features are encoded as \"feature_id:feature_value\" pairs, and\n",
        "      feature_id starts from 1.\n",
        "\n",
        "The pool of available articles for recommendation for each user visit is the set of articles that appear in that line of data.  All user IDs (specifically, bcookies) are replaced by a common string 'user' so that no user information can be identified from this data.\n",
        "\n",
        "Each user or article is associated with six features.  Feature #1 is the constant (always 1) feature, and features #2-6 correspond to the 5 membership features constructed via conjoint analysis with a bilinear model.\n",
        "\n",
        "A unique property of this data set is that the displayed article is chosen uniformly at random from the candidate article pool.  Therefore, one can use an unbiased *offline* evaluation method to compare bandit algorithms in a reliable way.  Performance of some of the popular bandit algorithms can be found. We will cover the offline evaluation method in detail in week 4 of the course.\n",
        "\n",
        "The full dataset can be downloaded from:\n",
        "https://webscope.sandbox.yahoo.com/catalog.php?datatype=r&did=49\n",
        "We are going to work with a much smalled sample of this data, which can be downloaded from the Google drive link (data1.txt):\n",
        "https://drive.google.com/drive/folders/10LGZMgXRuz2qPr_QDbYdVVlKEcnQ25YL?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1jHSMYulSPK4vC3zLgEM_E1boC4gKMriA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cbVPocoCvgQ",
        "outputId": "653a460f-cebe-482c-b914-c8910924cbb6"
      },
      "id": "2cbVPocoCvgQ",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jHSMYulSPK4vC3zLgEM_E1boC4gKMriA\n",
            "To: /content/yahoo.txt\n",
            "100% 158M/158M [00:01<00:00, 138MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d57d965-645f-4622-b721-7cd11500fc67",
      "metadata": {
        "id": "8d57d965-645f-4622-b721-7cd11500fc67"
      },
      "source": [
        "Now lets write a function to read this file, and store the data in articles / features / events -- which we will treat as global variables accessible to all functions later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a362ca4c-5f65-4f51-99c7-43b62fe6043f",
      "metadata": {
        "id": "a362ca4c-5f65-4f51-99c7-43b62fe6043f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import fileinput\n",
        "\n",
        "def read_data(filenames):\n",
        "    \"\"\"\n",
        "    Reads a stream of events from the list of given files.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    filenames : list\n",
        "        List of filenames\n",
        "    \n",
        "    Stores\n",
        "    -------    \n",
        "    articles : [article_ids]\n",
        "    features : [[article_1_features] .. [article_n_features]]\n",
        "    events : [\n",
        "                 0 : displayed_article_index (relative to the pool),\n",
        "                 1 : user_click,\n",
        "                 2 : [user_features],\n",
        "                 3 : [pool_indexes]\n",
        "             ]\n",
        "    \"\"\"\n",
        "\n",
        "    global articles, features, events, n_arms, n_events\n",
        "    articles = []\n",
        "    features = []\n",
        "    events = []\n",
        "\n",
        "    skipped = 0\n",
        "\n",
        "    with fileinput.input(files=filenames) as f:\n",
        "        for line in f:\n",
        "            cols = line.split()\n",
        "            if (len(cols) - 10) % 7 != 0:\n",
        "                skipped += 1\n",
        "            else:\n",
        "                pool_idx = []\n",
        "                pool_ids = []\n",
        "                for i in range(10, len(cols) - 6, 7):\n",
        "                    id = cols[i][1:]\n",
        "                    if id not in articles:\n",
        "                        articles.append(id)\n",
        "                        features.append([float(x[2:]) for x in cols[i + 1: i + 7]])\n",
        "                    pool_idx.append(articles.index(id))\n",
        "                    pool_ids.append(id)\n",
        "\n",
        "                events.append(\n",
        "                    [\n",
        "                        pool_ids.index(cols[1]),\n",
        "                        int(cols[2]),\n",
        "                        [float(x[2:]) for x in cols[4:10]],\n",
        "                        pool_idx,\n",
        "                    ]\n",
        "                )\n",
        "    features = np.array(features)\n",
        "    n_arms = len(articles)\n",
        "    n_events = len(events)\n",
        "    print(n_events, \"events with\", n_arms, \"articles\")\n",
        "    if skipped != 0:\n",
        "        print(\"Skipped events:\", skipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "888460da-4e01-4168-9573-761cf686786f",
      "metadata": {
        "id": "888460da-4e01-4168-9573-761cf686786f"
      },
      "source": [
        "Now lets read the file with the above defined function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "07df3304-6701-47ff-9d0d-d5a59a5b8cb3",
      "metadata": {
        "id": "07df3304-6701-47ff-9d0d-d5a59a5b8cb3",
        "outputId": "d46459b4-6476-4955-f9b2-f00c10dfad16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000 events with 21 articles\n"
          ]
        }
      ],
      "source": [
        "read_data(('yahoo.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e2039e-43ba-4a09-bbc9-b2b12687e722",
      "metadata": {
        "id": "95e2039e-43ba-4a09-bbc9-b2b12687e722"
      },
      "source": [
        "## œµ-greedy policy implementation\n",
        "Having read the files, now lets implement a simple œµ-greedy method. Here œµ defines the amount of exploration we wish to perform, and for the remaining 1-œµ times the model prefers exploitation. By convention, ‚Äúepsilon‚Äù represents the percentage of time/trials dedicated for exploration, and it is also typical to do random exploration. This introduces some form of stochasticity.\n",
        "\n",
        "The choose_arm function returns the best arm's index based on the œµ-greedy policy.\n",
        "The update functiom updates algorithm's parameters(matrices) for the selected arm. It looks at the selected arm, updates the number of times this arm was chosen, and then it updates the mean reward observed for the selected arm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d74ce40f-1fda-4397-ae7d-21f6faeacad9",
      "metadata": {
        "id": "d74ce40f-1fda-4397-ae7d-21f6faeacad9"
      },
      "outputs": [],
      "source": [
        "class Egreedy:\n",
        "    \"\"\"\n",
        "    Epsilon greedy algorithm implementation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        epsilon : number (Egreedy parameter, ideally between 0 and 1)\n",
        "        \"\"\"\n",
        "\n",
        "        self.e = round(epsilon, 1)  # epsilon parameter for Egreedy \n",
        "        self.algorithm = \"Egreedy (Œµ=\" + str(self.e) + \")\"\n",
        "        self.q = np.zeros(n_arms)  # average reward for each arm -- this represents the known mean reward for each arm\n",
        "        self.n = np.zeros(n_arms)  # number of times each arm was chosen\n",
        "\n",
        "    def choose_arm(self, t, user, pool_idx):\n",
        "        \"\"\"\n",
        "        Returns the best arm's index relative to the pool\n",
        "        Parameters\n",
        "        ----------\n",
        "        t : number (number of trial)\n",
        "        user : array (user features)\n",
        "        pool_idx : array of indexes (pool indexes for article identification)\n",
        "        \"\"\"\n",
        "\n",
        "        p = np.random.rand()\n",
        "        if p > self.e:\n",
        "            return np.argmax(self.q[pool_idx])\n",
        "        else:\n",
        "            return np.random.randint(low=0, high=len(pool_idx))\n",
        "\n",
        "    def update(self, displayed, reward, user, pool_idx):\n",
        "        \"\"\"\n",
        "        Updates algorithm's parameters(matrices)\n",
        "        Parameters\n",
        "        ----------\n",
        "        displayed : index (displayed article index relative to the pool)\n",
        "        reward : binary (user clicked or not)\n",
        "        user : array (user features)\n",
        "        pool_idx : array of indexes (pool indexes for article identification)\n",
        "        \"\"\"\n",
        "\n",
        "        a = pool_idx[displayed]\n",
        "        \n",
        "        # update counts pulled for chosen arm\n",
        "        self.n[a] += 1\n",
        "        \n",
        "        # update average/mean value/reward for chosen arm\n",
        "        self.q[a] += (reward - self.q[a]) / self.n[a]\n",
        "        \"\"\"\n",
        "        this can also be written as:\n",
        "        value = self.q[a]\n",
        "        new_value = ((self.n[a]-1)/float(self.n[a])) * value + (1 / float(self.n[a])) * reward\n",
        "        self.q[a] = new_value\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ca41bc-b6b7-49c3-867b-50bc836ccae3",
      "metadata": {
        "id": "13ca41bc-b6b7-49c3-867b-50bc836ccae3"
      },
      "source": [
        "# Policy evaluation to evaluate contextual bandits\n",
        "\n",
        "We base our evaluation on the seminal work on offline evaluation of bandits as presented in:\n",
        "- [WSDM 2011] Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms\n",
        "https://arxiv.org/pdf/1003.5956.pdf\n",
        "\n",
        "Compared to machine learning in the more standard supervised learning setting, evaluation of methods in a contextual bandit setting is frustratingly difficult. Our goal here is to measure the performance of a bandit algorithm A, that is, a rule for selecting an arm at each time step based on the preceding interactions and current context.\n",
        "\n",
        "We suppose that there is some unknown distribution D from which tuples are drawn i.i.d. of the form (x, r1, . . . , rK), each consisting of observed context and unobserved payoffs for all arms. We also posit access to a long sequence of logged events resulting from the interaction of the uniformly random logging policy with the world. Each such event consists of the context vector x, a selected arm a, and the resulting observed payoff ra. Crucially, this logged data is partially labeled in the sense that only the payoff ra is observed for the single arm a that was chosen uniformly at random.\n",
        "\n",
        "Our goal is to use this data to evaluate a bandit algorithm A, which is a (possibly randomized) mapping for selecting the arm at at time t based on the history ht‚àí1 of t‚àí1 preceding events together with the current context. Therefore, the data serves as a benchmark, with which people can evaluate and compare different bandit algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854372ec-b76f-4c85-a30f-e303979cb9f2",
      "metadata": {
        "id": "854372ec-b76f-4c85-a30f-e303979cb9f2"
      },
      "source": [
        "The policy evaluator is shown in Algorithm 1 above. The method takes as input a bandit algorithm A and a desired\n",
        "number of ‚Äúvalid‚Äù events T on which to base the evaluation. We then step through the stream of logged events one by one. If, given the current history ht‚àí1, it happens that the policy A chooses the same arm a as the one that was selected by the logging policy, then the event is retained (that is, added to the history), and the total payoff updated. Otherwise, if the policy A selects a different arm from the one that was taken by the logging policy, then the event is entirely ignored, and the algorithm proceeds to the next event without any change in its state.\n",
        "\n",
        "We next implement this evaluation function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3eb3334e-253f-483c-9e39-4bdda2c3b00e",
      "metadata": {
        "id": "3eb3334e-253f-483c-9e39-4bdda2c3b00e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "def evaluate(A, size=100, learn_ratio = 0.9):\n",
        "    \"\"\"\n",
        "    Policy evaluator as described in the paper\n",
        "    Parameters\n",
        "    ----------\n",
        "    A : class (algorithm)\n",
        "    size : number (run the evaluation only on a portion of the dataset)\n",
        "    learn_ratio : number (perform learning(update parameters) only on a small portion of the traffic)\n",
        "    Returns\n",
        "    -------\n",
        "    learn : array (contains the ctr for each trial for the learning bucket)\n",
        "    deploy : array (contains the ctr for each trial for the deployment bucket)\n",
        "    \"\"\"\n",
        "    \n",
        "    start = time.time()\n",
        "    # we initialize the payoff and events parameters separately for learning phase of the events and deployment phase of events.\n",
        "    Payoff_deploy = 0 # total payoff for the deployment bucket\n",
        "    Payoff_learn = 0  # total payoff for the learning bucket\n",
        "    Events_deploy = 1 # counter of valid events for the deployment bucket\n",
        "    Events_learn = 0  # counter of valid events for the learning bucket\n",
        "\n",
        "    learn = []\n",
        "    deploy = []\n",
        "    global events\n",
        "    if size != 100:\n",
        "        k = int(n_events * size / 100)\n",
        "        events = random.sample(events, k)\n",
        "\n",
        "    \"\"\"\n",
        "    we run through the logged events, and treat each event either for learning & updating the parameters,\n",
        "    or for deployment purposes wherein we use the reward obtained as evaluation metric\n",
        "    \"\"\"\n",
        "    for t, event in enumerate(events):\n",
        "\n",
        "        displayed = event[0]\n",
        "        reward = event[1]\n",
        "        user = event[2]\n",
        "        pool_idx = event[3]\n",
        "\n",
        "        # select the arm based on the bandit policy\n",
        "        chosen = A.choose_arm(Payoff_learn + Payoff_deploy, user, pool_idx)\n",
        "        \n",
        "        \"\"\"\n",
        "        If, given the current history ht‚àí1, it happens that the policy A chooses the same arm a\n",
        "        as the one that was selected by the logging policy, then the event is retained\n",
        "        (that is, added to the history), and the total payoff updated.\n",
        "        Otherwise, if the policy A selects a different arm from the one that was taken by the logging policy,\n",
        "        then the event is entirely ignored, and the algorithm proceeds to the next event without any change in its state.\n",
        "        \"\"\"\n",
        "        if chosen == displayed:\n",
        "            if random.random() < learn_ratio:\n",
        "                Payoff_learn += event[1]\n",
        "                Events_learn += 1\n",
        "                A.update(displayed, reward, user, pool_idx)\n",
        "                learn.append(Payoff_learn / Events_learn)\n",
        "            else:\n",
        "                Payoff_deploy += event[1]\n",
        "                Events_deploy += 1\n",
        "                deploy.append(Payoff_deploy / Events_deploy)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    execution_time = round(end - start, 1)\n",
        "    execution_time = (\n",
        "        str(round(execution_time / 60, 1)) + \"m\"\n",
        "        if execution_time > 60\n",
        "        else str(execution_time) + \"s\"\n",
        "    )\n",
        "    print(\n",
        "        \"{:<20}{:<10}{}\".format(\n",
        "            A.algorithm, round(Payoff_deploy / Events_deploy, 4), execution_time\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return learn, deploy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45aa4ece-69ca-4538-a88c-92e9b18a7510",
      "metadata": {
        "id": "45aa4ece-69ca-4538-a88c-92e9b18a7510"
      },
      "source": [
        "Now lets run the evaluation method on the epsilon-greedy policy for different values of epsilon, and print the corresponding reward obtained during deployment phase of evaluation.\n",
        "## Reward based evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d226bd62-a6bd-4068-ab5d-4e2bfdf4bb9e",
      "metadata": {
        "id": "d226bd62-a6bd-4068-ab5d-4e2bfdf4bb9e",
        "outputId": "c9a46bd8-5dd7-4cfd-a66a-89df01b62de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Egreedy (Œµ=0.1)     0.0325    0.7s\n",
            "Egreedy (Œµ=0.1)     0.0302    0.8s\n",
            "Egreedy (Œµ=0.1)     0.0455    0.7s\n",
            "Egreedy (Œµ=0.2)     0.0213    0.7s\n",
            "Egreedy (Œµ=0.5)     0.0363    0.7s\n",
            "Egreedy (Œµ=0.8)     0.0314    0.7s\n"
          ]
        }
      ],
      "source": [
        "_, deploy = evaluate(Egreedy(0.1),learn_ratio=0.25)\n",
        "rnd_ctr = deploy[-1]\n",
        "\n",
        "_, deploy = evaluate(Egreedy(0.1),learn_ratio=0.5)\n",
        "rnd_ctr = deploy[-1]\n",
        "\n",
        "_, deploy = evaluate(Egreedy(0.1),learn_ratio=0.9)\n",
        "rnd_ctr = deploy[-1]\n",
        "\n",
        "_, deploy = evaluate(Egreedy(0.25),learn_ratio=0.5)\n",
        "rnd_ctr = deploy[-1]\n",
        "\n",
        "_, deploy = evaluate(Egreedy(0.5),learn_ratio=0.5)\n",
        "rnd_ctr = deploy[-1]\n",
        "\n",
        "_, deploy = evaluate(Egreedy(0.75),learn_ratio=0.5)\n",
        "rnd_ctr = deploy[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0cfb8e-7c33-4f09-b917-ca26ea091142",
      "metadata": {
        "id": "bd0cfb8e-7c33-4f09-b917-ca26ea091142"
      },
      "source": [
        "# Goal 2: finish the implementation of UCB1 algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8bd37be-bf33-4613-b736-d674dc6d5889",
      "metadata": {
        "id": "b8bd37be-bf33-4613-b736-d674dc6d5889"
      },
      "source": [
        "## UCB-1 algorithm for arm selection\n",
        "\n",
        "Epsilon greedy performs pretty well, but it‚Äôs easy to see how selecting arms at random can be inefficient. If you have one movie that 50% of users have liked, and another at 5% have liked, epsilon greedy is equally likely to pick either of these movies when exploring random arms. Upper Confidence Bound algorithms were introduced as a class of bandit algorithm that explores more efficiently.\n",
        "\n",
        "Upper Confidence Bound algorithms construct a confidence interval of what each arm‚Äôs true performance might be, factoring in the uncertainty caused by variance in the data and the fact that we‚Äôre only able to observe a limited sample of pulls for any given arm. The algorithms then optimistically assume that each arm will perform as well as its upper confidence bound (UCB), selecting the arm with the highest UCB.\n",
        "\n",
        "### Motivation behind UCB\n",
        "Support an Artice A has been seen 100 times and has the best CTR. Article B has a slightly worse CTR than article A, but it hasn‚Äôt been seen by as many users, so there‚Äôs also more uncertainty about how well it‚Äôs going to perform in the long run. For this reason, it has a larger confidence bound, giving it a slightly higher UCB score than article A. Article C was published just moments ago, so almost no users have seen it. We‚Äôre extremely uncertain about how high its CTR will ultimately be, so its UCB is highest of all for now despite its initial CTR being low.\n",
        "\n",
        "Over time, more users will see articles B and C, and their confidence bounds will become more narrow and look more like that of article A. As we learn more about B and C, we‚Äôll shift from exploration toward exploitation as the articles‚Äô confidence intervals collapse toward their means. Unless the CTR of article B or C improves, the bandit will quickly start to favor article A again as the other articles‚Äô confidence bounds shrink."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a209f8cb-1753-4f0c-b855-6ba8c1d99d9f",
      "metadata": {
        "id": "a209f8cb-1753-4f0c-b855-6ba8c1d99d9f"
      },
      "source": [
        "Let $n_t(a)$ be the number of times arm a is selected in rounds 1,2,‚Ä¶,t and $\\mu_t(a)$ be the average reward of arm a up to time t. The upper confidence bound is defined as:\n",
        "\n",
        "$UCB_t(a) = \\mu_t(a)+ \\sqrt{\\frac{2 \\log(t)}{ N_k(t)}}$\n",
        "\n",
        "\n",
        "where $\\mu_t(a)$ can be computed as `rewards[k] / pulls[k]` and the bound can be computed as `sqrt((2 * log(t)) / pulls[k]`.\n",
        "\n",
        "The UCB1 algorithm chooses the best arm based on an optimistic estimate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f5881aa5-6396-469d-a582-95af04e2224a",
      "metadata": {
        "id": "f5881aa5-6396-469d-a582-95af04e2224a"
      },
      "outputs": [],
      "source": [
        "class Ucb1:\n",
        "    def __init__(self, alpha):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha : number (ucb parameter)\n",
        "        \"\"\"\n",
        "\n",
        "        self.alpha = round(alpha, 1)\n",
        "        self.algorithm = \"UCB1 (Œ±=\" + str(self.alpha) + \")\"\n",
        "\n",
        "        self.q = np.zeros(n_arms)  # average reward for each arm\n",
        "        self.n = np.ones(n_arms)  # number of times each arm was chosen\n",
        "\n",
        "    def choose_arm(self, t, user, pool_idx):\n",
        "        \"\"\"\n",
        "        Returns the best arm's index relative to the pool\n",
        "        Parameters\n",
        "        ----------\n",
        "        t : number (number of trial)\n",
        "        user : array (user features)\n",
        "        pool_idx : array of indexes (pool indexes for article identification)\n",
        "        \"\"\"\n",
        "        ucb = self.q[pool_idx]/self.n[pool_idx] + (np.sqrt(2 * np.log(t)) / self.n[pool_idx])\n",
        "        return np.argmax(ucb)\n",
        "\n",
        "    def update(self, displayed, reward, user, pool_idx):\n",
        "        \"\"\"\n",
        "        Updates algorithm's parameters(matrices)\n",
        "        Parameters\n",
        "        ----------\n",
        "        displayed : index (displayed article index relative to the pool)\n",
        "        reward : binary (user clicked or not)\n",
        "        user : array (user features)\n",
        "        pool_idx : array of indexes (pool indexes for article identification)\n",
        "        \"\"\"\n",
        "\n",
        "        a = pool_idx[displayed]\n",
        "\n",
        "        # update self.n[a] here\n",
        "        self.n[a] += 1\n",
        "\n",
        "        # update self.q[a] here\n",
        "        self.q[a] += (reward - self.q[a]) / self.n[a]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d34852c-8ccf-4455-9863-5a5acc8e56f9",
      "metadata": {
        "id": "1d34852c-8ccf-4455-9863-5a5acc8e56f9"
      },
      "source": [
        "One of the most important features of the UCB is that it not only exponentially decays as the number of pulls on the given machine increases, but also increases as the timestep increases. In other words, arms that have been explored less are given a boost even if their estimated mean is low, especially if we‚Äôve been playing for a while. In this way, the UCB1 algorithm is able to naturally define its own mix of exploration vs. exploitation without depending on a user supplied parameter like epsilon greedy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289e5976-b260-4df9-a00c-5ffe78ae46f0",
      "metadata": {
        "id": "289e5976-b260-4df9-a00c-5ffe78ae46f0"
      },
      "source": [
        "### Goal 2 for this week's project\n",
        "Finish the implementation of the UCB1 class above, and evaluate it for a set of parameters and compare its performance with respect to œµ-greedy policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "44ac4c75-1e33-48f4-a3bb-fcd8cea76d77",
      "metadata": {
        "id": "44ac4c75-1e33-48f4-a3bb-fcd8cea76d77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0367bebb-0d92-4aad-bb0f-0cbe40a62a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in sqrt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UCB1 (Œ±=0.1)        0.0346    2.5s\n"
          ]
        }
      ],
      "source": [
        "_, deploy = evaluate(Ucb1(0.1))\n",
        "rnd_ctr = deploy[-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Zq6hKgnKTUN"
      },
      "id": "8Zq6hKgnKTUN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-6.m89",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "name": "week2-multiTask-Bandit-final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a54c3793c5f4e1baef6f49bc6c03135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_970414b4a58c4239b89cb28fdc84b200",
              "IPY_MODEL_d57a547d456d441b967a61dce5e0c03f",
              "IPY_MODEL_7758f9acf37041deace83a93d4741e45"
            ],
            "layout": "IPY_MODEL_885318011acb4a21a2b5891ef29a7b85"
          }
        },
        "970414b4a58c4239b89cb28fdc84b200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17f08e7c0dc44739e49f87c0c358973",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_649ca2144ede4338b50006c9473907fc",
            "value": "100%"
          }
        },
        "d57a547d456d441b967a61dce5e0c03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_278e7447d8b240628eb1044a7a44d3aa",
            "max": 40000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fd8d734fd3c4194a0dd1e9d540608fc",
            "value": 40000
          }
        },
        "7758f9acf37041deace83a93d4741e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77eb115198e47e798bc6b0a305f9b5d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_80046eb2d2fb40b683314759177bbafe",
            "value": " 40000/40000 [00:04&lt;00:00, 10076.26it/s]"
          }
        },
        "885318011acb4a21a2b5891ef29a7b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d17f08e7c0dc44739e49f87c0c358973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "649ca2144ede4338b50006c9473907fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "278e7447d8b240628eb1044a7a44d3aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd8d734fd3c4194a0dd1e9d540608fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c77eb115198e47e798bc6b0a305f9b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80046eb2d2fb40b683314759177bbafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbb01d2b74374d8e85a03a1545285700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0211cc97a40d4fa7b00d8b382b088387",
              "IPY_MODEL_dead86576d55423bbc704d7708dd0086",
              "IPY_MODEL_20c7132abb49432eae782547e5a6dd2b"
            ],
            "layout": "IPY_MODEL_bcefc03e7ea341bbb3e361c79dbefb93"
          }
        },
        "0211cc97a40d4fa7b00d8b382b088387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22a2dd67b9dc4d7499782ec56ecbae7c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8d96b8b4a42b437bab13bffa49c19581",
            "value": "100%"
          }
        },
        "dead86576d55423bbc704d7708dd0086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfb5fc60fc524d3fad01507296147cff",
            "max": 40000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d8b67018c364bdfbfa16728edc0bc4f",
            "value": 40000
          }
        },
        "20c7132abb49432eae782547e5a6dd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37b6ef386dd94847a5f1bedc72d8a635",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_edc0167e788b4118a129ea389ed85fb8",
            "value": " 40000/40000 [00:04&lt;00:00, 10789.65it/s]"
          }
        },
        "bcefc03e7ea341bbb3e361c79dbefb93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a2dd67b9dc4d7499782ec56ecbae7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d96b8b4a42b437bab13bffa49c19581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfb5fc60fc524d3fad01507296147cff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d8b67018c364bdfbfa16728edc0bc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37b6ef386dd94847a5f1bedc72d8a635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edc0167e788b4118a129ea389ed85fb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
